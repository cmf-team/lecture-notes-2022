\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english,russian]{babel}
\usepackage{amsmath}

%графика
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{tikz}


\usepackage{tcolorbox}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{fixltx2e}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\usepackage{geometry}
\geometry{left=25mm,right=25mm,
 top=25mm,bottom=25mm}

\title{Data Science.\\
Lectures. Week 1. \\
Введение в эконометрику.}
\author{Матевосова Анастасия}

% Колонтитулы
\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0.1mm}  
\renewcommand{\footrulewidth}{0.1mm}
\lfoot{}
\rfoot{\thepage}
\cfoot{}
\rhead{CMF-2022}
\chead{}

\begin{document}
\maketitle

% Оглавление
\setcounter{tocdepth}{3} 
\renewcommand\contentsname{Contents}
\tableofcontents
\newpage


\section{Что такое эконометрика?}

\begin{itemize}
\item \textbf{Эконометрика} - это наука, которая изучает количественные и качественные экономические взаимосвязи с помощью математических и статистических методов и моделей.
\item \textbf {Эконометрика} - самостоятельная научная дисциплина, объединяющая совокупность теоретических результатов, методов и приёмов, позволяющих на базе \underline{экономической теории}, \underline {экономической информации} и \underline {математико-статистического инструментария} придавать конкретное количественное выражение общим (качественным) закономерностям, обусловленным экономической теорией.
\begin{flushright}
    С. Айвазян
\end{flushright} 
\item \textbf {3 составные части эконометрики:}
\begin{enumerate}
\item \textit{Теоретико-методологическая} - Экономическая теория
\item \textit {Информационная} - Экономические данные
\item \textit {Инструментальная} - Методы обработки данных
\end{enumerate}
\item \textit {Зачем нужна эконометрика?}

\textbf {Основные цели и задачи эконометрических методов:}
\begin{enumerate}
\item \underline {Задача прогнозирования} 

Пример: прогноз основных экономических показателей
\item \underline {Задача построения моделей для экономических систем}

Пример: имитация различных возможных сценариев социально-экономического развития страны.
\item \underline {Дескриптивный анализ}

Пример: конкретный статистический анализ рынка
\end{enumerate}

\section{Типы данных в эконометрике}

\item \textbf {3 типа данных в эконометрике:}
\begin{enumerate}
\item {Пространственные данные} 
\item Временные ряды
\item Панельные данные
\end{enumerate}



\item\textbf {Показатели (переменные):} 
\begin{enumerate}
\item{объясняющие переменные (регрессоры)} 
\item{зависимые или результирующие переменные}  
\end{enumerate}

\item \textbf{ \underline{Пространственные данные:}} в один и тот же момент времени (или промежуток времени) данные снимаются с случайно выбранных объектов. Т.е. объекты рассматриваются в пространстве.

Есть генеральная совокупность объектов. Каждый объект характеризуется набором показателей (переменных). 
Из генеральной совокупности извлекаются объекты случайным образом и получается случайная выборка $\xi _{1}$,...,$\xi _{n}$

С $i$-го объекта снимаются значения объясняющих переменных и результирующей переменной. $i$=1,...,n

X &= \begin{pmatrix}
           x^{(1)}_{i} \\
           x^{(2)}_{i} \\
           \vdots \\
           x^{(k)}_{i}
         \end{pmatrix} - набор показателей, который играет роль объясняющих переменных

$y_{i}$ - результирующая переменная (случай одной результирующей переменной)

Результатом является массив данных M=\{$(X_i, y_i), \;  i=1,..,n$\}

\item \textbf{ \underline{Временные ряды:}} имеем только один объект, снимаем значения показателей, характеризующих этот объект в последовательные моменты времени.

Результатом является массив данных M=\{$(X_t, y_t), \;  t=1,..,T$\} - этот массив представляет собой совокупность временных рядов.

\item \textbf{Принципиальная разница между пространственными данными и временными рядами:} 

\item[-]В случае \underline{пространственных данных} 
 как правило считаем, что данные собраны независимо друг от друга. Это случайная выборка. Результаты наблюдений за следующими объектами никак связаны с результатами, полученными в предыдущих наблюдениях. Т.е. в пространственных выборках были \underline{независимые одинаково распределённые случайные величины}. 

\item[-]В случае \underline{временного ряда}: существенная связь между предыдущими и следующими наблюдениями. \underline{Случайные величины зависимы и их законы распределения меняются во времени.}

\item \textbf{ \underline{Панельные данные данные:}} Смесь первых двух типов данных. Имеем n случайно выбранных объектов, каждый из которых наблюдается в течение некоторой последовательности моментов времени.

С $i$-го объекта в $t$-ый момент времени снимаем значения показателей: M = \{$(X_{it}, y_{it}), \;  i=1,..,n ; \; t=1,...,T$\}

\section{Пространственные данные}
\subsection{Модель парной регрессии}
M=\{$(x_i, y_i), \;  i=1,..,n$\} - массив - двумерный вектор, значения которого сняты со случайно взятых объектов

$x_i$ - единственная объясняющая переменная, снятая с $i$-го объекта

$y_i$ - результирующая переменная, снятая с $i$-го объекта

Предполагаем: $y$ связан с $x$ какой-то зависимостью
\fbox{$y_i=f(x_i, \beta)+\varepsilon_i$}

$\beta$-список параметров

$\varepsilon$ - случайная ошибка (аддитивная случайная компонента, в которой отражены все дополнительные переменные, влияющие на результрующую переменную).

Дополнительные факторы (переменные) - переменные, влияющие на результрующую переменную $y$ помимо объясняющей переменной $x$, являющиеся менее существенными. Дополнительные переменные ненаблюдаемы, в отличие от $x$ и $y$.

\subsubsection{Линейная модель парной регрессии (ЛМПР)}
$f$ является линейной относительно $x$.

\fbox{$y_i = \beta_1 + \beta_2x_i + \varepsilon_i$} - ЛМПР

($i=1,...,n$)

$\beta_1$ - свободный коэффициент

$\beta_2$ - угловой коэффициент

Надо оценить параметры $\beta_1 , \beta_2$. Наиболее распространённый метод-метод наименьших квадратов. 

\subsubsection{Метод наименьших квадратов}
$y_i = \beta_1 + \beta_2x_i + \varepsilon_i$

$\beta_1 , \beta_2$ -неизвестны, хотим оценить. Вместо $\beta_1$ будем подставлять произвольное число $b_1$, вместо $\beta_2$ подставляем $b_2$.

Нас интересуют значения $b_1, b_2$, дающие наиболее хорошую подгонку для неизвестной линейной связи.

Ищем $b_1^*, b_2^*$-оптимальные значения, которые \underline{минимизируют сумму квадратов отклонений}

\fbox{$\sum\limits_{i=1}^n(y_i-(b_1+b_2x_i))^2=Q(b_1,b_2) \rightarrow \underset{b_1,b_2}{min}$}

$b_1^*, b_2^*$ - функции от всего массива

$b_1^*=\hat{\beta_1}_{,MHK} = \overline{y} - \hat{\beta_2}\cdot\overline{x}$

$b_2^*=\hat{\beta_2}_{,MHK}=\frac{\overline{xy}-\overline{x}\cdot\overline{y}}{\overline{x^2}-(\overline{x})^2}$


\item Считаем, что работаем с данными, которые можно аппроксимировать линейной функцией.

В модели необходимо учесть все существенные факторы, поэтому нельзя ограничиваться моделью парной регрессии.

\subsection{Линейная модель множественной регрессии}
M=\{$(X_i, y_i), \;  i=1,..,n$\} - массив из несколькох объясняющих факторов и одного результирующего.
$X_i$-объясняющие переменные;

$y_i$-результирующая переменная.

\underline{Предполагаем, что связь линейная:}
\fbox{$y_i = \beta_1x_i^{(1)} + \beta_2x_i^{(2)} + ...+\beta_kx_i^{(k)}+\varepsilon_i$}

Правильность модели выражается в том, что:

\item В среднем влияние дополнительных переменных при фиксировванных $x$ отсутствует.

\fbox{$E(\varepsilon_i|x_i^{(1)},...,x_i^{(k)})=0$}

\qquad\qquad $\Downarrow$

\fbox{$E(y_i|X_i)=\beta_1x_i^{(1)} + \beta_2x_i^{(2)} + ...+\beta_kx_i^{(k)}$}

\item Выбранная модель хороша в том смысле, что в среднем мы угадали, что $y$ линейная функция от $x^{(1)},\ldots,x^{(k)}$ с ошибками, которые невелируются.


\vspace{3ex}
Обозначения:

\textit{$E(\varepsilon_i|x_i^{(1)},...,x_i^{(k)})$-условное мат.ожидание $\varepsilon_i$ при фиксированных значениях всех переменных $x_i^{(1)},...,x_i^{(k)}$}

\textit{$E(y_i|X_i)$-условное мат.ожидание переменной $y_i$ при условии, что все переменные $X_i (x_i^{(1)},...,x_i^{(k)})$ зафиксированы на каких-то уровнях.}

\section{Объясняющие переменные (регрессоры)}
\item\underline{\textbf{Объясняющие переменные:}}
\begin{enumerate}
\item детерминированные 
\item случайные
\end{enumerate}

\item \underline{Детерминированные}: Контролируемый эксперимент, в котором сами задаём значения х (объясняющей переменной).
Т.е. заказываем очередное наблюдение, в котором объясняющая переменная фиксированна так, как мы хотим.

Структура данных, которые таким образом будут собираться - системы вертикальных точек.

\item \underline{Случайные}: Получаем конкретные данные и не можем управлять значениями х (объясняющей переменной).

Структура данных - хаотичная, нет вертикальной структуры.

В этом случае интерпретируем х, как случайную величину (случайный регрессор).

\subsection{Детерминированные объясняющие переменные}
\subsubsection{Свойства оценок $\hat{\beta}$, полученных с помощью МНК для ЛМПР}

$M_n$=\{$(X_i, y_i), \;  i=1,..,n$\} - массив из n наблюдений (объём выборки = n)

$\hat{\beta}(M_n)$ - оценка для параметра $\beta$, как функция от массива. $\hat{\beta}(n)$, т.е. зависит от $n$.

\item Если $x_i$-детерминированы, то $y_i$ случайны (т.к. в модели есть случайные ошибки и поэтому переменная $y$ является случайной величиной). 

Каждый из $y_1, y_2, ... , y_n$ - случайная величина

\vspace{2ex}
Считаем, что все наборы объясняющих переменных - детерминированы. Т.е. интерпретируются как числа, а не как случайные величины.


$\hat{\beta_1}_{,MHK} = \overline{y} - \hat{\beta_2}\cdot\overline{x}$

$\hat{\beta_2}_{,MHK}=\frac{\overline{xy}-\overline{x}\cdot\overline{y}}{\overline{x^2}-(\overline{x})^2} = c_1y_1+c_2y_2+...+c_ny_n$

$c_i=\frac{\frac{1}{n}(x_i-\overline{x})}{\overline{x^2}-(\overline{x})^2} = const$ по $y_1, ..., y_n$


\item \underline{Обе оценки являются \textbf{линейными функциями относительно $y$}}

\vspace{2ex}

\item \textbf{Свойства хороших оценок:}
\item[-]\underline{\textbf{Несмещённая}}: \fbox{$E\hat{\beta_2}_{,MHK}=\hat{\beta_2} \quad \forall n$} (для МНК существуют условия, при которых это свойство будет верным)
\item[-]\underline{\textbf{Состоятельная}}: \fbox{$\hat{\beta}(M_n) \xrightarrow[n\rightarrow\infty]{P} \beta$} (сходимость по вероятности)

сходимость по вероятности=состоятельность : \fbox{\forall $\delta$ \; P\{$|\hat{\beta(n)}-\beta|>\delta$\}$\xrightarrow[n\rightarrow\infty]{}0$}

\underline{Состоятельность - ассимптотическое свойство.} Поэтому когда наблюдений немного, оценка может очень сильно отклоняться от оцениваемого параметра. При большом объёме выборки с большой уверенностью можно сказать, что состоятельная оценка окажется очень хорошим аналогом для значения параметра $\beta$

\item[-]\underline{\textbf{Эффективная (оптимальная)}}:
\begin{enumerate}
    \item \underline{Относительная эффективность}:
    Из двух несмещённых оценок одна относительно эффективнее другой, если её дисперсия не больше дисперсии другой.

$\hat{\beta}$ относительно эффективнее $\widetilde{\beta}$, если $D\hat{\beta} \leq D\widetilde{\beta}$ \;\; ($E(\hat{\beta}-\beta)^2=D\hat{\beta}$)
    
    \item \underline{Абсолютная эффективность}: Эффективность в классе

    Рассматривается класс несмещённых оценок
    $K=\{\widetilde{\beta} : E\widetilde{\beta}=\beta \}$. В этом классе одна конкретная оценка $\hat{\beta}$ называется эффективной, если $\forall\widetilde{\beta} \in K \; $ $D\hat{\beta} \leq D\widetilde{\beta}$
\end{enumerate}
\vspace{2ex}
Несмещённость и эффективность рассматриваются при фиксированном $n$. Состоятельность - это асимптотическое свойство (т.е. при $n\rightarrow\infty$).

\subsubsection{Условия, при которых МНК даёт хорошие оценки. Теорема Гаусса-Маркова.}
Если
\item $E\varepsilon_i=0 \qquad i=1,...,n$             \item $D\varepsilon_i=\sigma^2=const \qquad i=1,...,n$ (одинаковая, не зависящая от номера наблюдения)
\item $cov(\varepsilon_i,\varepsilon_j)=0 \qquad i\neq j;\; i=1,...,n;\; j=1,...,n$,

то МНК-оценки коэффициентов $\beta_1, \beta_2$ в ЛМПР будут:
\item несмещёнными;
\item эффективными в классе всех несмещённых, линейных по $y_1,...,y_n$ оценок.

\vspace{4ex}
Т.е. для $\beta_2$ рассматриваем класс $K_2=\{\widetilde{\beta_2}=c_1y_1+...+c_ny_n$, где $c_1,..,c_n$ не зависят от $y$, при этом $E\widetilde{\beta_2}=\beta\}$.

Теорема Гаусса-Маркова утверждает, что $\hat{\beta_2}_{,MHK},\in K_2$ и $D\hat{\beta_2}_{,MHK}\leq D\widetilde{\beta_2} \quad \forall \widetilde{\beta_2} \in K_2$.

То же самое верно для $\hat{\beta_1}_{,MHK}$

Эти МНК-оценки называются BLUE - Best Linear Unbiased Estimator.

Для сформулированной теоремы: Рассматриваются ЛМПР, считая, что эта модель правильная и что нет других существенных переменных, кроме $х$, которые влияют на $y$. \\ $y_i=\beta_1+\beta_2x_i+\varepsilon_i$
(т.е. безошибочно задали спецификацию модели).

В случае использования ЛМПР и неучёта существенных переменных, эти МНК оценки не будут обладать хорошими свойствами.

\subsubsection{ЛММР}
Три формы записи:
\begin{enumerate}
    \item $y_i = \beta_1x_i^{(1)} + \beta_2x_i^{(2)} + ...+\beta_kx_i^{(k)}+\varepsilon_i \qquad i=1,...,n$
    \item \underline{Векторная форма}: $y_i = \beta^Tx_i+\varepsilon_i \qquad \beta &= \begin{pmatrix}
           \beta_1 \\
           \vdots \\
           \beta_k
         \end{pmatrix} \qquad x_i &= \begin{pmatrix}
           x^{(1)}_{i} \\
           \vdots \\
           x^{(k)}_{i}
         \end{pmatrix}$ 
    \item \underline{Векторно-матричная форма}: $y=X\beta+\varepsilon \qquad y &= \begin{pmatrix}
           y_1 \\
           \vdots \\
           y_n
         \end{pmatrix}$ 
         $\qquad \beta &= \begin{pmatrix}
           \beta_1 \\
           \vdots \\
           \beta_k
         \end{pmatrix}$ 
         $\quad X &= \begin{pmatrix}
           x^{(1)}_1 \; x^{(2)}_1 ... x^{(k)}_1\\
           x^{(1)}_2 \; x^{(2)}_2 ... x^{(k)}_2\\
           \vdots \\
           x^{(1)}_n \; x^{(2)}_n ... x^{(k)}_n\\
         \end{pmatrix}$ \\
         $y(n\times1)$-набор всех наблюдений за переменной $y$\\
         $\beta(k\times1)$\\
         $X(n\times k)$ -матрица плана \qquad ($i$-ая строчка - наблюдения, снятые с $i$-го объекта)
         
         $X$ называется \textbf{матрицей плана}, потому что если рассматривается управляемый планируемый эксперимент, когда мы сами назначаем значения $x$, то всю матрицу мы должны зафиксировать.

         $(X,y)$ - массив $M$
\end{enumerate}

\vspace{2ex}
\subsubsection{Вектор МНК-оценок}
$\hat{\beta}_{MHK}$=\begin{pmatrix}
           \hat{\beta_1}_{,MHK} \\
           \hat{\beta_2}_{,MHK}\\
           \vdots \\
           \hat{\beta_k}_{,MHK}
         \end{pmatrix} - вектор МНК-оценок

\vspace{2ex}
 Зафиксируем матрицу $X$, получим случайным образом реализации $у$ на всех n объектах, тем самым мы получили массив данных. Затем мы можем повторить процедуру получения массива, не меняя матрицу $X$, но получая новую реализацию вектора $y$. Таких реализаций можно сделать много и получить огромное количество массивов с одним и тем же $X$ и разными $y$.\\
\vspace{1ex}
$\hat{\beta_i}_{,MHK}$ является функцией от массива М.\\
$M=(X,y)$, где $X$-фиксирован, $y$-случайный (меняется)
\\Возьмём большое количество реализаций массива $М$ (например, $10^6$), будут получаться разные векторы МНК-оценок параметров.\\
Для $10^6$ реализаций мы получим $10^6$ реализаций случайной величины $E\hat{\beta_i}_{,MHK}$.\\ 
В соотвествии с теоремой Гаусса-Маркова:
$E\hat{\beta_i}_{,MHK}=\beta_i$  \qquad (говорит о том, что оценка хорошая).\\

\vspace{2ex}
Как посчитать вектор МНК-оценок?\\
\fbox{$\hat{\beta}_{MHK}=(X^TX)^{-1}X^Ty$}

По умолчанию требуется условие $rank(X)=k$
\vspace{1ex}
\item Всегда ли существует $(X^TX)^-1$?\\
$(X^TX)^-1$ не существует, когда $det(X^TX)=0 \Leftrightarrow rank(X)<k$\\
Это означает, что одна из объясняющих переменных $x^{(1)}... x^{(k)}$является линейной комбинацией остальных. А это означает, что нет смысла включать в модель все k переменных: ту, которая линейно выражается через остальные можно выкинуть, тем самым сократив количество регрессоров и сделав полный ранг.
\end{itemize}
\vspace{2ex}
Свойства вектора МНК-оценок.
\begin{enumerate}
\item МНК оценка является \textbf{линейной оценкой по $y$}:\\
$A=(X^TX)^{-1}X^T$\\
$\hat{\beta}_{MHK}=Ay$
\item \textbf{Несмещённость}: $E\hat{\beta}_{MHK}=\beta$
\item \textbf{Теорема Гаусса-Маркова}: В классе линейных по $y$ и несмещённых оценок МНК оценки имеют наименьшую дисперсию.\\
В классе $K_j$=\{$\widetilde{\beta_j}=c_1y_1+...+c_ny_n$ - несмещённые\} \quad $\forall\widetilde{\beta_j} \in K_j \; $ $D\hat{\beta}_j_{,MHK} \leq D\widetilde{\beta_j}$
\end{enumerate}

 \vspace{4ex}
\subsubsection{Коэффициент детерминации $R^2$}
Меры разброса:
\begin{itemize}
\item \fbox{$TSS=\sum\limits_{i=1}^n(y_i-\overline{y})^2$} -общая сумма квадратов\\
\item \fbox{$\frac{TSS}{n}=\hat{Var}(y)$} - выборочная дисперсия\\


\vspace{1ex}

Рассматриваем модель $y=X\beta+\varepsilon$ , применяем МНК $\Rightarrow$ получаем $\hat{\beta}$\\
$y-X\hat{\beta}=\hat{\varepsilon}=e$ - вектор оценки случайной ошибки
\vspace{2ex}
Модель $y_i = \beta_1x_i^{(1)} + \beta_2x_i^{(2)} + ...+\beta_kx_i^{(k)}+\varepsilon_i$\\
Применили МНК и получили: $\hat{\beta}_1,...,\hat{\beta}_k$\\
$\hat{y}_i = \hat{\beta}_1x_i^{(1)} + \hat{\beta}_2x_i^{(2)} + ...+\hat{\beta}_kx_i^{(k)}$\\ $\hat{y}_i$ - оценка $y_i$, рассчитанное значение $y$ на $i$-ом объекте.\\
$y_i$-наблюдаемая реализация случайной величины $y$ на $i$-ом объекте.\\
$y_i-\hat{y}_i=\hat{\varepsilon}_i=e_i$\\
$\varepsilon_i$-регрессионная случайная ошибка\\
$\hat{\varepsilon}_i=e_i$ - регрессионный остаток (является случайной величиной, так как $y$ случайны).\\

Разница между случайной ошибкой ($\varepsilon_i$) и остатком ($\hat{\varepsilon}_i$) состоит в том, что:\\ случайная ошибка ($\varepsilon_i$) - ненаблюдаемая случайная величина,\\ остаток ($\hat{\varepsilon}_i$) - вычисленная, то есть наблюдаемая случайная величина.
\vspace{2ex}

$TSS=\sum\limits_{i=1}^n(y_i-\overline{y})^2$\\

$\overline{\hat{y}}=\frac{1}{n}\sum\limits_{i=1}^n(\hat{y_i})=\overline{y}$\\
$RSS=\sum\limits_{i=1}^n(\hat{y}_i-\overline{\hat{y}})^2=\sum\limits_{i=1}^n(\hat{y}_i-\overline{y})^2$ \\
\item \fbox{$RSS=\sum\limits_{i=1}^n(\hat{y}_i-\overline{y})^2$} - Regression sum of squares\\
\item \fbox{$ESS=\sum\limits_{i=1}^n(y_i-\hat{y}_i)^2=e_i^2$} - Error sum of squares\\
$ESS=\frac{1}{n}\sum\limits_{i=1}^n(e_i-\overline{e})^2=\frac{1}{n}\sum\limits_{i=1}^ne_i^2$ \\
$e_i=y_i-\hat{y}_i$ - число\\
$e=y-\hat{y}$ - вектор\\
$\overline{e}=\overline{y}-\overline{\hat{y}}=0$ - число

\vspace{2ex}
$TSS$-мера разброса $y$ вокруг своего центра;\\
$RSS$-мера разброса $\hat{y}$;\\
$ESS$-мера малости остатков.

\vspace{2ex}
\textbf{Теорема}\\
Если модель содержит свободный коэффициент, т.е. записывается в виде:\\
$y_i = \beta_1 + \beta_2x_i^{(2)} + ...+\beta_kx_i^{(k)}$, \quad т.е. $x^{(1)}_i \equiv 1$,\\
то $TSS=RSS+ESS$\\

\vspace{1ex}
Таким образом, $TSS$ разбивается на две части $RSS$ и $ESS$, одна в другую перетекает.\\
Если остатки очень маленькие, то $ESS$ маленькая величина (хорошее качество модели), значит доля $RSS$ в общей сумме большая.\\
Поэтому вводится мера качества ЛММР: $R^2=1-\frac{ESS}{RSS}$ - чем больше, тем лучше\\

\vspace{1ex}
\fbox{R^2=1-\frac{ESS}{RSS}} - \textbf{коэффициент детерминации} - мера качества линейной модели множественной регрессии
\vspace{1ex}

\textbf{Свойства $R^2$ (в рамках теоремы - т.е. при наличии свободного коэффициента)}:
\item $0 \leqslant R^2\leqslant 1$\\
Для моделей без свободного коэффициента может быть неверным.
\item $R^2=1 \Leftrightarrow ESS=0 \Leftrightarrow$ все $e_i=0 \Leftrightarrow$ все $y_i=\hat{y_i}$\\
$R^2=1$ означает, что никаких случайных ошибок нет, т.е. абсолютно точная подгонка с помощью линейной функции.\\
Если $R^2 \approx 1 \quad (0,8-0,99)$ - это говорит о хорошей подгонке модели, т.е. ошибки очень маленькие, \textbf{качество модели хорошее}.\\
Если облако точек подгоняется с помощью прямой линии очень хорошо, то это значит, что размазанность вокруг этой прямой линии маленькая. 
\item $R^2=0$:
$1-\frac{ESS}{TSS}=0$\\
$TSS=ESS$\\
$RSS=0$\\
Т.е. горизонтальное облако. Нет связи. Свидетельствует о \textbf{плохой модели}\\

\vspace{1ex}
\underline{Таким образом, $R^2$ является мерой качества модели: чем ближе он к 1, тем лучше.}



\end{itemize}

 

\end{document}
